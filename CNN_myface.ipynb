{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dir = 'c:/users/ximzz/desktop/face/crop'\n",
    "height=[]\n",
    "width=[]\n",
    "for root, dirs, files in os.walk(search_dir):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        image_files = 'c:/users/ximzz/desktop/face/crop/'+file\n",
    "        image = cv2.imread(image_files)\n",
    "        height.append(image.shape[0])\n",
    "        width.append(image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "696\n"
     ]
    }
   ],
   "source": [
    "print(int(np.mean(height)))\n",
    "print(int(np.mean(width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(search_dir):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        image_files = 'c:/users/ximzz/desktop/face/crop/'+file\n",
    "        image = cv2.imread(image_files)\n",
    "        resized = cv2.resize(image,(400,243))\n",
    "        cv2.imwrite('c:/users/ximzz/desktop/face/crop/resize/'+file, resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0162671.jpg\n",
      "0164780.jpg\n",
      "0281626.jpg\n",
      "0282585.jpg\n",
      "0625584.jpg\n",
      "1.jpg\n",
      "10.jpg\n",
      "11.jpg\n",
      "12.jpg\n",
      "1273975.jpg\n",
      "12741023.jpg\n",
      "1380735.jpg\n",
      "1381683.jpg\n",
      "1452458.jpg\n",
      "1453511.jpg\n",
      "16071107.jpg\n",
      "1672581.jpg\n",
      "1674610.jpg\n",
      "2.jpg\n",
      "3.jpg\n",
      "4.jpg\n",
      "5.jpg\n",
      "6.jpg\n",
      "7.jpg\n",
      "8.jpg\n",
      "9.jpg\n",
      "9872822.jpg\n"
     ]
    }
   ],
   "source": [
    "face_data = []\n",
    "for root, dirs, files in os.walk('c:/users/ximzz/desktop/face/crop/resize'):\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        image_files = 'c:/users/ximzz/desktop/face/crop/resize/'+file\n",
    "        image = cv2.imread(image_files)\n",
    "        face_data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_array = np.array(face_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 983, 768, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((27,2),dtype=np.int32)\n",
    "y[[5,6,7,8,18,19,20,21,22,23,24,25],[0]] =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[[0,1,2,3,4,9,10,11,12,13,14,15,16,17,26],[1]]=1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epoch= 15\n",
    "batch_size =100\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "training= True\n",
    "\n",
    "X= tf.placeholder(tf.float32, shape=[None, 983,768,3 ])\n",
    "Y= tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs = X, filters = 32, kernel_size=[3,3], padding='SAME',\n",
    "        activation = tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size=[2,2],\n",
    "                        padding = 'SAME', strides=2)\n",
    "dropout1 = tf.layers.dropout(inputs=pool1, rate=0.7,\n",
    "                training=training)\n",
    "\n",
    "# after shape=(492,384)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs =dropout1 , filters = 64, kernel_size=[3,3], padding='SAME',\n",
    "                                    activation = tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs = conv2, pool_size=[2,2],\n",
    "                                       padding = 'SAME', strides=2)\n",
    "dropout2 = tf.layers.dropout(inputs=pool2, rate=0.7,\n",
    "                                        training=training) \n",
    "# after shape =(246, 192)\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs = dropout2, filters = 128, kernel_size=[3,3], padding='SAME',\n",
    "                                    activation = tf.nn.relu) \n",
    "pool3 = tf.layers.max_pooling2d(inputs = conv3, pool_size=[2,2],\n",
    "                                       padding = 'SAME', strides=2)\n",
    "dropout3 = tf.layers.dropout(inputs=pool3, rate=0.7,\n",
    "                                        training=training) #트루면 먹이고 false면 안맥임\n",
    "\n",
    "#after shape =(123,96)\n",
    "\n",
    "flat = tf.reshape(dropout3, [-1,123*96*128])\n",
    "dense4 = tf.layers.dense(inputs = flat, units = 625, activation=tf.nn.relu)\n",
    "dropout4 = tf.layers.dropout(inputs=dense4,rate=0.5,\n",
    "                                        training=training)\n",
    "            \n",
    "logits = tf.layers.dense(inputs = dropout4, units=2)\n",
    "\n",
    "cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=Y))\n",
    "optimizer= tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "def predict(x_test, training=False):\n",
    "            return sess.run(logits, feed_dict={X:x_test,\n",
    "                                                     training:training})\n",
    "    \n",
    "    \n",
    "def get_accuracy(x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy, feed_dict={X:x_test, Y:y_test, \n",
    "                                                       training:training})\n",
    "    \n",
    "def train(x_data, y_data, training=True):\n",
    "        return sess.run([cost, optimizer], feed_dict={X:x_data,\n",
    "                                                            Y:y_data,\n",
    "                                                            training:training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 cost :  231.243\n",
      "Epoch : 1 cost :  507998.0\n",
      "Epoch : 2 cost :  246265.0\n",
      "Epoch : 3 cost :  63255.8\n",
      "Epoch : 4 cost :  2470.19\n",
      "Epoch : 5 cost :  21949.2\n",
      "Epoch : 6 cost :  20994.6\n",
      "Epoch : 7 cost :  12366.3\n",
      "Epoch : 8 cost :  6598.73\n",
      "Epoch : 9 cost :  2942.59\n",
      "Epoch : 10 cost :  954.159\n",
      "Epoch : 11 cost :  305.107\n",
      "Epoch : 12 cost :  264.809\n",
      "Epoch : 13 cost :  169.739\n",
      "Epoch : 14 cost :  302.527\n",
      "Epoch : 15 cost :  315.558\n",
      "Epoch : 16 cost :  332.309\n",
      "Epoch : 17 cost :  326.422\n",
      "Epoch : 18 cost :  309.841\n",
      "Epoch : 19 cost :  221.541\n",
      "Epoch : 20 cost :  318.341\n",
      "Epoch : 21 cost :  194.295\n",
      "Epoch : 22 cost :  238.955\n",
      "Epoch : 23 cost :  99.6144\n",
      "Epoch : 24 cost :  114.575\n",
      "Epoch : 25 cost :  143.641\n",
      "Epoch : 26 cost :  87.8698\n",
      "Epoch : 27 cost :  86.7536\n",
      "Epoch : 28 cost :  87.3168\n",
      "Epoch : 29 cost :  53.712\n",
      "Epoch : 30 cost :  89.9391\n",
      "Epoch : 31 cost :  48.8513\n",
      "Epoch : 32 cost :  45.3831\n",
      "Epoch : 33 cost :  33.263\n",
      "Epoch : 34 cost :  41.0722\n",
      "Epoch : 35 cost :  28.7823\n",
      "Epoch : 36 cost :  38.4579\n",
      "Epoch : 37 cost :  47.5932\n",
      "Epoch : 38 cost :  23.153\n",
      "Epoch : 39 cost :  36.7632\n",
      "Epoch : 40 cost :  19.6627\n",
      "Epoch : 41 cost :  16.7137\n",
      "Epoch : 42 cost :  26.3128\n",
      "Epoch : 43 cost :  23.5148\n",
      "Epoch : 44 cost :  17.1863\n",
      "Epoch : 45 cost :  6.68953\n",
      "Epoch : 46 cost :  9.52388\n",
      "Epoch : 47 cost :  20.2235\n",
      "Epoch : 48 cost :  10.2838\n",
      "Epoch : 49 cost :  18.6999\n",
      "Epoch : 50 cost :  12.9582\n",
      "Epoch : 51 cost :  20.2089\n",
      "Epoch : 52 cost :  15.8235\n",
      "Epoch : 53 cost :  12.5717\n",
      "Epoch : 54 cost :  4.76946\n",
      "Epoch : 55 cost :  8.44935\n",
      "Epoch : 56 cost :  5.20576\n",
      "Epoch : 57 cost :  4.94145\n",
      "Epoch : 58 cost :  6.68378\n",
      "Epoch : 59 cost :  4.07752\n",
      "Epoch : 60 cost :  5.79838\n",
      "Epoch : 61 cost :  8.39833\n",
      "Epoch : 62 cost :  10.1295\n",
      "Epoch : 63 cost :  10.4107\n",
      "Epoch : 64 cost :  3.52113\n",
      "Epoch : 65 cost :  3.0171\n",
      "Epoch : 66 cost :  7.77461\n",
      "Epoch : 67 cost :  2.27204\n",
      "Epoch : 68 cost :  0.700482\n",
      "Epoch : 69 cost :  9.7436\n",
      "Epoch : 70 cost :  2.6056\n",
      "Epoch : 71 cost :  7.62013\n",
      "Epoch : 72 cost :  2.76598\n",
      "Epoch : 73 cost :  2.78059\n",
      "Epoch : 74 cost :  9.54933\n",
      "Epoch : 75 cost :  9.54161\n",
      "Epoch : 76 cost :  12.2537\n",
      "Epoch : 77 cost :  5.59481\n",
      "Epoch : 78 cost :  9.11502\n",
      "Epoch : 79 cost :  2.47577\n",
      "Epoch : 80 cost :  3.36839\n",
      "Epoch : 81 cost :  5.92713\n",
      "Epoch : 82 cost :  6.13959\n",
      "Epoch : 83 cost :  6.19649\n",
      "Epoch : 84 cost :  3.35224\n",
      "Epoch : 85 cost :  6.3762\n",
      "Epoch : 86 cost :  12.9453\n",
      "Epoch : 87 cost :  5.24037\n",
      "Epoch : 88 cost :  6.2575\n",
      "Epoch : 89 cost :  3.50916\n",
      "Epoch : 90 cost :  5.69214\n",
      "Epoch : 91 cost :  4.5991\n",
      "Epoch : 92 cost :  6.58464\n",
      "Epoch : 93 cost :  0.591069\n",
      "Epoch : 94 cost :  5.55737\n",
      "Epoch : 95 cost :  2.62202\n",
      "Epoch : 96 cost :  1.64112\n",
      "Epoch : 97 cost :  1.95076\n",
      "Epoch : 98 cost :  3.34812\n",
      "Epoch : 99 cost :  2.37778\n",
      "Epoch : 100 cost :  9.52489\n",
      "Epoch : 101 cost :  4.08519\n",
      "Epoch : 102 cost :  3.63705\n",
      "Epoch : 103 cost :  4.78693\n",
      "Epoch : 104 cost :  3.8609\n",
      "Epoch : 105 cost :  0.353273\n",
      "Epoch : 106 cost :  1.09253\n",
      "Epoch : 107 cost :  1.17084\n",
      "Epoch : 108 cost :  8.40055\n",
      "Epoch : 109 cost :  1.79895\n",
      "Epoch : 110 cost :  6.29952\n",
      "Epoch : 111 cost :  6.01384\n",
      "Epoch : 112 cost :  1.46876\n",
      "Epoch : 113 cost :  0.653012\n",
      "Epoch : 114 cost :  3.47847\n",
      "Epoch : 115 cost :  8.40843\n",
      "Epoch : 116 cost :  2.62366\n",
      "Epoch : 117 cost :  7.63211\n",
      "Epoch : 118 cost :  4.27902\n",
      "Epoch : 119 cost :  4.10203\n",
      "Epoch : 120 cost :  4.02322\n",
      "Epoch : 121 cost :  1.01791\n",
      "Epoch : 122 cost :  0.753899\n",
      "Epoch : 123 cost :  4.76168\n",
      "Epoch : 124 cost :  2.69618\n",
      "Epoch : 125 cost :  1.06149\n",
      "Epoch : 126 cost :  1.97422\n",
      "Epoch : 127 cost :  5.68002\n",
      "Epoch : 128 cost :  7.14061\n",
      "Epoch : 129 cost :  1.9462\n",
      "Epoch : 130 cost :  2.496\n",
      "Epoch : 131 cost :  5.80889\n",
      "Epoch : 132 cost :  1.72617\n",
      "Epoch : 133 cost :  3.9201\n",
      "Epoch : 134 cost :  12.9872\n",
      "Epoch : 135 cost :  6.33517\n",
      "Epoch : 136 cost :  2.25159\n",
      "Epoch : 137 cost :  1.67015\n",
      "Epoch : 138 cost :  0.820449\n",
      "Epoch : 139 cost :  0.682679\n",
      "Epoch : 140 cost :  1.61855\n",
      "Epoch : 141 cost :  1.08864e-05\n",
      "Epoch : 142 cost :  9.5473\n",
      "Epoch : 143 cost :  2.69127\n",
      "Epoch : 144 cost :  2.02472\n",
      "Epoch : 145 cost :  5.8368\n",
      "Epoch : 146 cost :  4.46741\n",
      "Epoch : 147 cost :  1.69681\n",
      "Epoch : 148 cost :  3.74032\n",
      "Epoch : 149 cost :  5.59858\n",
      "Epoch : 150 cost :  5.31266\n",
      "Epoch : 151 cost :  3.49239\n",
      "Epoch : 152 cost :  0.00157926\n",
      "Epoch : 153 cost :  0.483217\n",
      "Epoch : 154 cost :  1.18605\n",
      "Epoch : 155 cost :  4.26859\n",
      "Epoch : 156 cost :  0.000162121\n",
      "Epoch : 157 cost :  2.89649\n",
      "Epoch : 158 cost :  0.353115\n",
      "Epoch : 159 cost :  0.339773\n",
      "Epoch : 160 cost :  1.23161\n",
      "Epoch : 161 cost :  1.43869\n",
      "Epoch : 162 cost :  1.61618\n",
      "Epoch : 163 cost :  0.723902\n",
      "Epoch : 164 cost :  2.90021\n",
      "Epoch : 165 cost :  2.19626\n",
      "Epoch : 166 cost :  4.62274\n",
      "Epoch : 167 cost :  2.29722\n",
      "Epoch : 168 cost :  7.16014\n",
      "Epoch : 169 cost :  1.23675\n",
      "Epoch : 170 cost :  6.55785\n",
      "Epoch : 171 cost :  3.07384\n",
      "Epoch : 172 cost :  3.82661\n",
      "Epoch : 173 cost :  1.39875\n",
      "Epoch : 174 cost :  0.0892236\n",
      "Epoch : 175 cost :  6.9543\n",
      "Epoch : 176 cost :  0.64995\n",
      "Epoch : 177 cost :  2.26364\n",
      "Epoch : 178 cost :  1.28891\n",
      "Epoch : 179 cost :  2.315\n",
      "Epoch : 180 cost :  0.877333\n",
      "Epoch : 181 cost :  2.09891\n",
      "Epoch : 182 cost :  4.24269\n",
      "Epoch : 183 cost :  2.05064\n",
      "Epoch : 184 cost :  2.56078\n",
      "Epoch : 185 cost :  0.263483\n",
      "Epoch : 186 cost :  0.813471\n",
      "Epoch : 187 cost :  1.12018\n",
      "Epoch : 188 cost :  3.06417\n",
      "Epoch : 189 cost :  3.65876\n",
      "Epoch : 190 cost :  2.96528\n",
      "Epoch : 191 cost :  2.38696\n",
      "Epoch : 192 cost :  0.807579\n",
      "Epoch : 193 cost :  6.43866\n",
      "Epoch : 194 cost :  0.779513\n",
      "Epoch : 195 cost :  4.51938\n",
      "Epoch : 196 cost :  1.64038\n",
      "Epoch : 197 cost :  3.86494\n",
      "Epoch : 198 cost :  0.0120777\n",
      "Epoch : 199 cost :  0.997851\n",
      "Epoch : 200 cost :  1.91394\n",
      "Epoch : 201 cost :  2.30118\n",
      "Epoch : 202 cost :  1.75393\n",
      "Epoch : 203 cost :  1.60172\n",
      "Epoch : 204 cost :  0.498527\n",
      "Epoch : 205 cost :  2.23622\n",
      "Epoch : 206 cost :  1.50637\n",
      "Epoch : 207 cost :  1.58191\n",
      "Epoch : 208 cost :  3.54943\n",
      "Epoch : 209 cost :  1.50702\n",
      "Epoch : 210 cost :  0.396148\n",
      "Epoch : 211 cost :  1.24089e-05\n",
      "Epoch : 212 cost :  2.17556\n",
      "Epoch : 213 cost :  0.936857\n",
      "Epoch : 214 cost :  3.37872\n",
      "Epoch : 215 cost :  0.191419\n",
      "Epoch : 216 cost :  0.379651\n",
      "Epoch : 217 cost :  1.32234\n",
      "Epoch : 218 cost :  6.2099e-05\n",
      "Epoch : 219 cost :  1.34065\n",
      "Epoch : 220 cost :  3.24138\n",
      "Epoch : 221 cost :  2.84438\n",
      "Epoch : 222 cost :  2.31516\n",
      "Epoch : 223 cost :  2.1035\n",
      "Epoch : 224 cost :  5.41766\n",
      "Epoch : 225 cost :  5.22861\n",
      "Epoch : 226 cost :  0.564098\n",
      "Epoch : 227 cost :  3.17813\n",
      "Epoch : 228 cost :  4.44987\n",
      "Epoch : 229 cost :  1.58452\n",
      "Epoch : 230 cost :  2.04026\n",
      "Epoch : 231 cost :  0.864707\n",
      "Epoch : 232 cost :  0.190628\n",
      "Epoch : 233 cost :  2.85817\n",
      "Epoch : 234 cost :  1.04079\n",
      "Epoch : 235 cost :  1.2725\n",
      "Epoch : 236 cost :  0.34257\n",
      "Epoch : 237 cost :  1.11688\n",
      "Epoch : 238 cost :  0.264101\n",
      "Epoch : 239 cost :  1.7452\n",
      "Epoch : 240 cost :  0.2126\n",
      "Epoch : 241 cost :  5.44063\n",
      "Epoch : 242 cost :  1.26753\n",
      "Epoch : 243 cost :  0.252665\n",
      "Epoch : 244 cost :  1.35755\n",
      "Epoch : 245 cost :  0.945067\n",
      "Epoch : 246 cost :  2.85707\n",
      "Epoch : 247 cost :  2.63133\n",
      "Epoch : 248 cost :  3.8461\n",
      "Epoch : 249 cost :  1.87113\n",
      "Epoch : 250 cost :  4.68146\n",
      "Epoch : 251 cost :  1.80952\n",
      "Epoch : 252 cost :  1.63255\n",
      "Epoch : 253 cost :  1.51443\n",
      "Epoch : 254 cost :  1.02497\n",
      "Epoch : 255 cost :  1.69108e-05\n",
      "Epoch : 256 cost :  1.78426\n",
      "Epoch : 257 cost :  2.381\n",
      "Epoch : 258 cost :  1.88307\n",
      "Epoch : 259 cost :  1.39719\n",
      "Epoch : 260 cost :  1.43052\n",
      "Epoch : 261 cost :  0.0598958\n",
      "Epoch : 262 cost :  0.955792\n",
      "Epoch : 263 cost :  3.3145\n",
      "Epoch : 264 cost :  1.06369\n",
      "Epoch : 265 cost :  0.00283772\n",
      "Epoch : 266 cost :  1.44677\n",
      "Epoch : 267 cost :  1.02747\n",
      "Epoch : 268 cost :  1.85274\n",
      "Epoch : 269 cost :  2.95307\n",
      "Epoch : 270 cost :  6.61809\n",
      "Epoch : 271 cost :  2.29809\n",
      "Epoch : 272 cost :  0.674137\n",
      "Epoch : 273 cost :  3.81056\n",
      "Epoch : 274 cost :  3.95175e-05\n",
      "Epoch : 275 cost :  0.235356\n",
      "Epoch : 276 cost :  1.15255\n",
      "Epoch : 277 cost :  1.17062\n",
      "Epoch : 278 cost :  0.541312\n",
      "Epoch : 279 cost :  2.64254\n",
      "Epoch : 280 cost :  1.59504\n",
      "Epoch : 281 cost :  3.25755\n",
      "Epoch : 282 cost :  0.498999\n",
      "Epoch : 283 cost :  6.51783\n",
      "Epoch : 284 cost :  0.553436\n",
      "Epoch : 285 cost :  0.332381\n",
      "Epoch : 286 cost :  0.652756\n",
      "Epoch : 287 cost :  0.017565\n",
      "Epoch : 288 cost :  1.14489\n",
      "Epoch : 289 cost :  0.0955594\n",
      "Epoch : 290 cost :  2.57379\n",
      "Epoch : 291 cost :  0.839681\n",
      "Epoch : 292 cost :  1.88944\n",
      "Epoch : 293 cost :  1.11465\n",
      "Epoch : 294 cost :  1.7595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 295 cost :  1.35223\n",
      "Epoch : 296 cost :  0.366131\n",
      "Epoch : 297 cost :  1.30861\n",
      "Epoch : 298 cost :  0.651452\n",
      "Epoch : 299 cost :  1.25504\n",
      "Epoch : 300 cost :  3.05843\n",
      "Epoch : 301 cost :  0.000715272\n",
      "Epoch : 302 cost :  1.99392\n",
      "Epoch : 303 cost :  0.307354\n",
      "Epoch : 304 cost :  3.61898\n",
      "Epoch : 305 cost :  4.15415\n",
      "Epoch : 306 cost :  0.689469\n",
      "Epoch : 307 cost :  0.892004\n",
      "Epoch : 308 cost :  3.38359\n",
      "Epoch : 309 cost :  1.3122\n",
      "Epoch : 310 cost :  2.52095\n",
      "Epoch : 311 cost :  0.0\n",
      "Epoch : 312 cost :  1.38013\n",
      "Epoch : 313 cost :  1.07407\n",
      "Epoch : 314 cost :  3.23158\n",
      "Epoch : 315 cost :  1.28884\n",
      "Epoch : 316 cost :  0.888448\n",
      "Epoch : 317 cost :  1.18038\n",
      "Epoch : 318 cost :  0.620015\n",
      "Epoch : 319 cost :  0.68237\n",
      "Epoch : 320 cost :  2.55462\n",
      "Epoch : 321 cost :  2.07714\n",
      "Epoch : 322 cost :  2.44045\n",
      "Epoch : 323 cost :  1.01616\n",
      "Epoch : 324 cost :  0.655609\n",
      "Epoch : 325 cost :  0.140767\n",
      "Epoch : 326 cost :  0.826644\n",
      "Epoch : 327 cost :  0.605758\n",
      "Epoch : 328 cost :  1.26745\n",
      "Epoch : 329 cost :  0.742134\n",
      "Epoch : 330 cost :  1.05623\n",
      "Epoch : 331 cost :  2.15552\n",
      "Epoch : 332 cost :  2.9392\n",
      "Epoch : 333 cost :  0.247499\n",
      "Epoch : 334 cost :  1.37364\n",
      "Epoch : 335 cost :  2.02323\n",
      "Epoch : 336 cost :  0.575252\n",
      "Epoch : 337 cost :  0.0\n",
      "Epoch : 338 cost :  0.845605\n",
      "Epoch : 339 cost :  0.245121\n",
      "Epoch : 340 cost :  0.0163259\n",
      "Epoch : 341 cost :  0.364257\n",
      "Epoch : 342 cost :  0.897131\n",
      "Epoch : 343 cost :  0.0111801\n",
      "Epoch : 344 cost :  0.467982\n",
      "Epoch : 345 cost :  0.000623194\n",
      "Epoch : 346 cost :  0.595371\n",
      "Epoch : 347 cost :  2.23676\n",
      "Epoch : 348 cost :  0.767341\n",
      "Epoch : 349 cost :  1.7748\n",
      "Epoch : 350 cost :  1.34814\n",
      "Epoch : 351 cost :  2.2528\n",
      "Epoch : 352 cost :  1.69528\n",
      "Epoch : 353 cost :  0.997863\n",
      "Epoch : 354 cost :  1.46582\n",
      "Epoch : 355 cost :  1.96338\n",
      "Epoch : 356 cost :  1.36799\n",
      "Epoch : 357 cost :  6.09289e-07\n",
      "Epoch : 358 cost :  0.0191327\n",
      "Epoch : 359 cost :  0.32439\n",
      "Epoch : 360 cost :  0.42494\n",
      "Epoch : 361 cost :  1.85125\n",
      "Epoch : 362 cost :  0.615163\n",
      "Epoch : 363 cost :  0.881413\n",
      "Epoch : 364 cost :  0.0010225\n",
      "Epoch : 365 cost :  1.09259e-05\n",
      "Epoch : 366 cost :  0.462409\n",
      "Epoch : 367 cost :  0.0\n",
      "Epoch : 368 cost :  0.573526\n",
      "Epoch : 369 cost :  0.741537\n",
      "Epoch : 370 cost :  0.791522\n",
      "Epoch : 371 cost :  0.397381\n",
      "Epoch : 372 cost :  8.25543e-06\n",
      "Epoch : 373 cost :  1.83748\n",
      "Epoch : 374 cost :  1.95143\n",
      "Epoch : 375 cost :  2.8696\n",
      "Epoch : 376 cost :  0.0807441\n",
      "Epoch : 377 cost :  0.420573\n",
      "Epoch : 378 cost :  0.604275\n",
      "Epoch : 379 cost :  2.73453\n",
      "Epoch : 380 cost :  0.000325534\n",
      "Epoch : 381 cost :  2.59312\n",
      "Epoch : 382 cost :  0.765565\n",
      "Epoch : 383 cost :  0.000817753\n",
      "Epoch : 384 cost :  0.853306\n",
      "Epoch : 385 cost :  4.41516e-09\n",
      "Epoch : 386 cost :  1.15627\n",
      "Epoch : 387 cost :  0.0563012\n",
      "Epoch : 388 cost :  0.000145632\n",
      "Epoch : 389 cost :  1.30245e-06\n",
      "Epoch : 390 cost :  1.35346\n",
      "Epoch : 391 cost :  0.211776\n",
      "Epoch : 392 cost :  1.4566\n",
      "Epoch : 393 cost :  4.50344e-07\n",
      "Epoch : 394 cost :  0.0327439\n",
      "Epoch : 395 cost :  1.76159\n",
      "Epoch : 396 cost :  1.89329\n",
      "Epoch : 397 cost :  0.378359\n",
      "Epoch : 398 cost :  0.106683\n",
      "Epoch : 399 cost :  0.611051\n",
      "Epoch : 400 cost :  0.82887\n",
      "Epoch : 401 cost :  0.538071\n",
      "Epoch : 402 cost :  0.919087\n",
      "Epoch : 403 cost :  0.679769\n",
      "Epoch : 404 cost :  1.07488\n",
      "Epoch : 405 cost :  1.25489\n",
      "Epoch : 406 cost :  0.616419\n",
      "Epoch : 407 cost :  0.232123\n",
      "Epoch : 408 cost :  1.61358\n",
      "Epoch : 409 cost :  3.97364e-08\n",
      "Epoch : 410 cost :  2.60138\n",
      "Epoch : 411 cost :  1.85644\n",
      "Epoch : 412 cost :  0.0\n",
      "Epoch : 413 cost :  0.579029\n",
      "Epoch : 414 cost :  1.5431\n",
      "Epoch : 415 cost :  0.69268\n",
      "Epoch : 416 cost :  0.71618\n",
      "Epoch : 417 cost :  0.295685\n",
      "Epoch : 418 cost :  0.261542\n",
      "Epoch : 419 cost :  0.0785154\n",
      "Epoch : 420 cost :  0.180033\n",
      "Epoch : 421 cost :  0.157033\n",
      "Epoch : 422 cost :  2.0238e-05\n",
      "Epoch : 423 cost :  1.8248\n",
      "Epoch : 424 cost :  0.903179\n",
      "Epoch : 425 cost :  2.33454\n",
      "Epoch : 426 cost :  1.13936\n",
      "Epoch : 427 cost :  7.22895e-05\n",
      "Epoch : 428 cost :  8.16795e-07\n",
      "Epoch : 429 cost :  0.66473\n",
      "Epoch : 430 cost :  8.86458e-06\n",
      "Epoch : 431 cost :  0.461397\n",
      "Epoch : 432 cost :  0.245178\n",
      "Epoch : 433 cost :  0.112712\n",
      "Epoch : 434 cost :  2.11292\n",
      "Epoch : 435 cost :  0.278413\n",
      "Epoch : 436 cost :  4.85667e-08\n",
      "Epoch : 437 cost :  0.75083\n",
      "Epoch : 438 cost :  0.00392413\n",
      "Epoch : 439 cost :  0.641572\n",
      "Epoch : 440 cost :  0.0697754\n",
      "Epoch : 441 cost :  0.363492\n",
      "Epoch : 442 cost :  0.000189798\n",
      "Epoch : 443 cost :  0.00111337\n",
      "Epoch : 444 cost :  8.74194e-07\n",
      "Epoch : 445 cost :  0.611821\n",
      "Epoch : 446 cost :  1.44043\n",
      "Epoch : 447 cost :  6.14329e-05\n",
      "Epoch : 448 cost :  0.000160322\n",
      "Epoch : 449 cost :  0.858744\n",
      "Epoch : 450 cost :  0.769851\n",
      "Epoch : 451 cost :  0.779391\n",
      "Epoch : 452 cost :  0.121359\n",
      "Epoch : 453 cost :  5.60723e-07\n",
      "Epoch : 454 cost :  6.61441e-05\n",
      "Epoch : 455 cost :  0.0168129\n",
      "Epoch : 456 cost :  0.0\n",
      "Epoch : 457 cost :  0.68088\n",
      "Epoch : 458 cost :  4.81249e-07\n",
      "Epoch : 459 cost :  0.542748\n",
      "Epoch : 460 cost :  1.09318\n",
      "Epoch : 461 cost :  9.48256e-06\n",
      "Epoch : 462 cost :  0.0527863\n",
      "Epoch : 463 cost :  2.90215e-05\n",
      "Epoch : 464 cost :  1.88965e-06\n",
      "Epoch : 465 cost :  1.84549e-06\n",
      "Epoch : 466 cost :  0.0449307\n",
      "Epoch : 467 cost :  0.0101885\n",
      "Epoch : 468 cost :  0.304946\n",
      "Epoch : 469 cost :  0.0\n",
      "Epoch : 470 cost :  0.0\n",
      "Epoch : 471 cost :  0.00168082\n",
      "Epoch : 472 cost :  3.63351e-06\n",
      "Epoch : 473 cost :  1.45775\n",
      "Epoch : 474 cost :  1.26012\n",
      "Epoch : 475 cost :  1.27517\n",
      "Epoch : 476 cost :  0.253063\n",
      "Epoch : 477 cost :  9.57965e-06\n",
      "Epoch : 478 cost :  0.102211\n",
      "Epoch : 479 cost :  2.2559\n",
      "Epoch : 480 cost :  1.50687\n",
      "Epoch : 481 cost :  1.34078\n",
      "Epoch : 482 cost :  0.0602973\n",
      "Epoch : 483 cost :  0.188553\n",
      "Epoch : 484 cost :  0.92398\n",
      "Epoch : 485 cost :  0.100482\n",
      "Epoch : 486 cost :  0.00262689\n",
      "Epoch : 487 cost :  3.50106e-06\n",
      "Epoch : 488 cost :  0.367006\n",
      "Epoch : 489 cost :  0.462534\n",
      "Epoch : 490 cost :  0.00110085\n",
      "Epoch : 491 cost :  0.735737\n",
      "Epoch : 492 cost :  0.572467\n",
      "Epoch : 493 cost :  1.03424\n",
      "Epoch : 494 cost :  0.323857\n",
      "Epoch : 495 cost :  0.0\n",
      "Epoch : 496 cost :  5.13011e-06\n",
      "Epoch : 497 cost :  1.18659\n",
      "Epoch : 498 cost :  0.839011\n",
      "Epoch : 499 cost :  1.7881e-06\n",
      "Epoch : 500 cost :  3.40393e-06\n",
      "Epoch : 501 cost :  0.988364\n",
      "Epoch : 502 cost :  0.149972\n",
      "Epoch : 503 cost :  1.76606e-08\n",
      "Epoch : 504 cost :  0.00633508\n",
      "Epoch : 505 cost :  3.19644e-06\n",
      "Epoch : 506 cost :  1.40171\n",
      "Epoch : 507 cost :  1.54678e-05\n",
      "Epoch : 508 cost :  2.77036e-05\n",
      "Epoch : 509 cost :  7.19664e-07\n",
      "Epoch : 510 cost :  0.332732\n",
      "Epoch : 511 cost :  1.01177\n",
      "Epoch : 512 cost :  0.00238444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-836cae11a3e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mface_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"cost : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(2000):\n",
    "\n",
    "    c,_ = sess.run([cost, optimizer], feed_dict={X: face_array, Y:y, keep_prob:0.5})\n",
    "    print(\"Epoch :\", epoch , \"cost : \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
